import streamlit as st
import google.generativeai as genai
from PIL import Image
import io
import base64
from datetime import datetime
import tiktoken
import json
import time
import re
import os
import logging
from cryptography.fernet import Fernet
import matplotlib.pyplot as plt

# C·∫•u h√¨nh logging
logging.basicConfig(filename='chatbot.log', level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

# C·∫•u h√¨nh trang
st.set_page_config(page_title="Tr√≤ chuy·ªán ƒêa ph∆∞∆°ng ti·ªán v·ªõi Gemini", layout="wide", page_icon="üöÄ")

# Kh·ªüi t·∫°o tr·∫°ng th√°i phi√™n
if "messages" not in st.session_state:
    st.session_state.messages = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "image" not in st.session_state:
    st.session_state.image = None
if "model_config" not in st.session_state:
    st.session_state.model_config = {
        "model_name": "gemini-1.5-flash-latest",
        "temperature": 0.7,
        "top_p": 1.0,
        "top_k": 40,
        "max_output_tokens": 2048,
    }
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch v√† th√¢n thi·ªán ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi tanbaycu. "
if "total_tokens" not in st.session_state:
    st.session_state.total_tokens = 0
if "theme" not in st.session_state:
    st.session_state.theme = "light"
if "font_size" not in st.session_state:
    st.session_state.font_size = "medium"
if "sessions" not in st.session_state:
    st.session_state.sessions = {}
if "current_session" not in st.session_state:
    st.session_state.current_session = "M·∫∑c ƒë·ªãnh"

GEMINI_MODELS = [
    "gemini-1.5-flash-latest",
    "gemini-2.0-flash-exp",
    "gemini-1.5-flash-8b",
    "gemini-2.0-pro-exp-02-05",
    "gemini-2.0-flash-lite-preview-02-05"
]

MAX_TOKENS = 8192  # Gi·∫£ s·ª≠ ƒë√¢y l√† gi·ªõi h·∫°n token cho m√¥ h√¨nh

# C·∫£i thi·ªán giao di·ªán ng∆∞·ªùi d√πng
def get_custom_css():
    return f"""
<style>
    .stButton > button {{
        width: 100%;
        transition: all 0.3s ease;
    }}
    .stButton > button:hover {{
        transform: translateY(-2px);
        box-shadow: 0 2px 5px rgba(0,0,0,0.2);
    }}
    .stTextInput > div > div > input {{
        background-color: {'#f0f2f6' if st.session_state.theme == 'light' else '#2b313e'};
    }}
    .sidebar .stButton > button {{
        background-color: #4CAF50;
        color: white;
    }}
    .sidebar .stButton > button:hover {{
        background-color: #45a049;
    }}
    .chat-message {{
        padding: 1rem; 
        border-radius: 0.5rem; 
        margin-bottom: 1rem; 
        display: flex;
        font-size: {{'0.8rem' if st.session_state.font_size == 'small' else '1rem' if st.session_state.font_size == 'medium' else '1.2rem'}};
        animation: fadeIn 0.5s;
    }}
    @keyframes fadeIn {{
        0% {{ opacity: 0; }}
        100% {{ opacity: 1; }}
    }}
    .chat-message.user {{
        background-color: {'#e6f3ff' if st.session_state.theme == 'light' else '#2b313e'};
    }}
    .chat-message.bot {{
        background-color: {'#f0f0f0' if st.session_state.theme == 'light' else '#3c4354'};
    }}
    .chat-message .avatar {{
        width: 15%;
        padding-right: 0.5rem;
    }}
    .chat-message .avatar img {{
        max-width: 40px;
        max-height: 40px;
        border-radius: 50%;
    }}
    .chat-message .message {{
        width: 85%;
        padding: 0 1.5rem;
    }}
    .chat-message .timestamp {{
        font-size: 0.8em;
        color: {'#a0a0a0' if st.session_state.theme == 'light' else '#cccccc'};
        text-align: right;
        margin-top: 0.5rem;
    }}
    .token-info {{
        font-size: 0.8em;
        color: {'#a0a0a0' if st.session_state.theme == 'light' else '#cccccc'};
        margin-top: 0.5rem;
    }}
    body {{
        transition: background-color 0.3s ease, color 0.3s ease;
        background-color: {'#ffffff' if st.session_state.theme == 'light' else '#1e1e1e'};
        color: {'#000000' if st.session_state.theme == 'light' else '#ffffff'};
    }}
    .stAlert {{
        animation: slideIn 0.5s;
    }}
    @keyframes slideIn {{
        0% {{ transform: translateY(-100%); }}
        100% {{ transform: translateY(0); }}
    }}
    
    /* C·∫£i thi·ªán kh·∫£ nƒÉng ph·∫£n h·ªìi tr√™n c√°c thi·∫øt b·ªã di ƒë·ªông */
    @media (max-width: 768px) {{
        .chat-message {{
            flex-direction: column;
        }}
        .chat-message .avatar {{
            width: 100%;
            margin-bottom: 0.5rem;
        }}
        .chat-message .message {{
            width: 100%;
        }}
    }}
</style>
"""

st.markdown(get_custom_css(), unsafe_allow_html=True)

# T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t
@st.cache_resource
def load_model(api_key, model_name):
    try:
        genai.configure(api_key=api_key)
        return genai.GenerativeModel(model_name=model_name)
    except Exception as e:
        logging.error(f"L·ªói kh·ªüi t·∫°o m√¥ h√¨nh: {str(e)}")
        st.error(f"L·ªói kh·ªüi t·∫°o m√¥ h√¨nh: {str(e)}")
        return None

@st.cache_data
def get_image_download_link(_img, filename, text):
    buffered = io.BytesIO()
    _img.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    href = f'<a href="data:file/png;base64,{img_str}" download="{filename}">{text}</a>'
    return href

@st.cache_data
def count_tokens(text):
    encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

# C·∫£i thi·ªán c∆° ch·∫ø gi·ªõi h·∫°n t·ªëc ƒë·ªô
def rate_limited_response(func):
    def wrapper(*args, **kwargs):
        if 'last_request_time' not in st.session_state:
            st.session_state.last_request_time = 0
        
        current_time = time.time()
        time_since_last_request = current_time - st.session_state.last_request_time
        
        if time_since_last_request < 0.5:  # Gi·ªõi h·∫°n 2 y√™u c·∫ßu m·ªói gi√¢y
            time.sleep(0.5 - time_since_last_request)
        
        result = func(*args, **kwargs)
        st.session_state.last_request_time = time.time()
        return result
    return wrapper

# Qu·∫£n l√Ω phi√™n
def save_session(session_name):
    st.session_state.sessions[session_name] = {
        "messages": st.session_state.messages,
        "chat_history": st.session_state.chat_history,
        "total_tokens": st.session_state.total_tokens,
        "model_config": st.session_state.model_config,
        "system_prompt": st.session_state.system_prompt
    }
    logging.info(f"ƒê√£ l∆∞u phi√™n '{session_name}'")
    st.success(f"ƒê√£ l∆∞u phi√™n '{session_name}'")

def load_session(session_name):
    if session_name in st.session_state.sessions:
        session_data = st.session_state.sessions[session_name]
        st.session_state.messages = session_data["messages"]
        st.session_state.chat_history = session_data["chat_history"]
        st.session_state.total_tokens = session_data["total_tokens"]
        st.session_state.model_config = session_data["model_config"]
        st.session_state.system_prompt = session_data["system_prompt"]
        st.session_state.current_session = session_name
        logging.info(f"ƒê√£ t·∫£i phi√™n '{session_name}'")
        st.success(f"ƒê√£ t·∫£i phi√™n '{session_name}'")
    else:
        logging.warning(f"Kh√¥ng t√¨m th·∫•y phi√™n '{session_name}'")
        st.error(f"Kh√¥ng t√¨m th·∫•y phi√™n '{session_name}'")

def search_chat_history(query):
    results = []
    for msg in st.session_state.chat_history:
        if query.lower() in msg['content'].lower():
            results.append(msg)
    return results

# X·ª≠ l√Ω l·ªói v√† ph·∫£n h·ªìi
def handle_error(error_message):
    logging.error(error_message)
    st.error(f"L·ªói: {error_message}")
    st.info("Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c li√™n h·ªá h·ªó tr·ª£ n·∫øu l·ªói v·∫´n ti·∫øp t·ª•c.")

# H√†m x·ª≠ l√Ω input c·ªßa ng∆∞·ªùi d√πng (ƒë√£ ƒë∆∞·ª£c c·∫£i ti·∫øn)
@rate_limited_response
def handle_user_input(user_input, model):
    start_time = time.time()
    sanitized_input = sanitize_input(user_input)
    chat_history = get_chat_history()
    full_prompt = f"{st.session_state.system_prompt}\n\nL·ªãch s·ª≠ tr√≤ chuy·ªán:\n{chat_history}\n\nNg∆∞·ªùi d√πng: {sanitized_input}\n\nTr·ª£ l√Ω:"
    
    inputs = [full_prompt]
    if st.session_state.image:
        inputs.append(st.session_state.image)
    
    try:
        with st.spinner('ü§î ƒêang t·∫°o ph·∫£n h·ªìi...'):
            response = model.generate_content(
                inputs,
                generation_config=genai.types.GenerationConfig(
                    temperature=st.session_state.model_config["temperature"],
                    top_p=st.session_state.model_config["top_p"],
                    top_k=st.session_state.model_config["top_k"],
                    max_output_tokens=st.session_state.model_config["max_output_tokens"],
                )
            )
        response_tokens = count_tokens(response.text)
        st.session_state.total_tokens += response_tokens
        processing_time = time.time() - start_time
        logging.info(f"Ph·∫£n h·ªìi ƒë∆∞·ª£c t·∫°o trong {processing_time:.2f} gi√¢y, s·ª≠ d·ª•ng {response_tokens} tokens")
        return response.text, response_tokens, processing_time
    except Exception as e:
        error_message = f"L·ªói t·∫°o ph·∫£n h·ªìi: {str(e)}"
        logging.error(error_message)
        handle_error(error_message)
        return None, 0, 0

def sanitize_input(text):
    # Lo·∫°i b·ªè c√°c th·∫ª HTML
    text = re.sub('<[^<]+?>', '', text)
    # Escape c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
    text = text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('"', '&quot;').replace("'", '&#39;')
    return text

def get_chat_history():
    return "\n".join([f"{msg['role']}: {msg['content']}" for msg in st.session_state.chat_history[-st.session_state.max_history:]])

# Thanh b√™n (v·ªõi c√°c c·∫£i ti·∫øn)
with st.sidebar:
    st.title("C√†i ƒë·∫∑t")
    
    if 'api_key' not in st.session_state:
        st.session_state.api_key = ''
    api_key = st.text_input("Nh·∫≠p Google API Key", type="password", value=st.session_state.api_key)
    
    # Ch·∫ø ƒë·ªô t·ªëi/s√°ng
    theme = st.radio("Ch·ªçn ch·ªß ƒë·ªÅ", ["S√°ng", "T·ªëi"])
    st.session_state.theme = "light" if theme == "S√°ng" else "dark"
    
    with st.expander("üõ†Ô∏è T√πy ch·ªânh M√¥ h√¨nh", expanded=False):
        selected_model = st.selectbox("Ch·ªçn m√¥ h√¨nh Gemini", GEMINI_MODELS, index=GEMINI_MODELS.index(st.session_state.model_config["model_name"]))
        st.session_state.model_config["model_name"] = selected_model
        
        st.session_state.model_config["temperature"] = st.slider("üå°Ô∏è ƒê·ªô s√°ng t·∫°o", min_value=0.0, max_value=1.0, value=st.session_state.model_config["temperature"], step=0.1)
        st.session_state.model_config["top_p"] = st.slider("üéØ Top P", min_value=0.0, max_value=1.0, value=st.session_state.model_config["top_p"], step=0.1)
        st.session_state.model_config["top_k"] = st.number_input("üîù Top K", min_value=1, max_value=100, value=st.session_state.model_config["top_k"])
        st.session_state.model_config["max_output_tokens"] = st.number_input("üìè S·ªë token t·ªëi ƒëa", min_value=1, max_value=8192, value=st.session_state.model_config["max_output_tokens"])
    
    with st.expander("üìù T√πy ch·ªânh Prompt", expanded=False):
        st.session_state.system_prompt = st.text_area("System Prompt", value=st.session_state.system_prompt, height=100)
    
    st.session_state.max_history = st.slider("üß† S·ªë l∆∞·ª£ng tin nh·∫Øn t·ªëi ƒëa trong l·ªãch s·ª≠", min_value=1, max_value=50, value=5)
    
    uploaded_file = st.file_uploader("üì∏ T·∫£i l√™n m·ªôt h√¨nh ·∫£nh...", type=["jpg", "jpeg", "png"])

    if uploaded_file:
        st.session_state.image = Image.open(uploaded_file)
        st.image(st.session_state.image, caption='H√¨nh ·∫£nh ƒë√£ t·∫£i l√™n', use_column_width=True)
        st.markdown(get_image_download_link(st.session_state.image, "h√¨nh_·∫£nh_ƒë√£_t·∫£i.png", "üì• T·∫£i xu·ªëng h√¨nh ·∫£nh"), unsafe_allow_html=True)

    # Qu·∫£n l√Ω phi√™n
    with st.expander("üíæ Qu·∫£n l√Ω phi√™n", expanded=False):
        session_name = st.text_input("T√™n phi√™n")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("L∆∞u phi√™n"):
                if session_name:
                    save_session(session_name)
                else:
                    st.warning("Vui l√≤ng nh·∫≠p t√™n phi√™n")
        with col2:
            if st.button("T·∫£i phi√™n"):
                if session_name:
                    load_session(session_name)
                else:
                    st.warning("Vui l√≤ng nh·∫≠p t√™n phi√™n")
        
        if st.session_state.sessions:
            selected_session = st.selectbox("Ch·ªçn phi√™n ƒë·ªÉ t·∫£i", list(st.session_state.sessions.keys()))
            if st.button("T·∫£i phi√™n ƒë√£ ch·ªçn"):
                load_session(selected_session)

        # T√¨m ki·∫øm trong l·ªãch s·ª≠ tr√≤ chuy·ªán
        search_query = st.text_input("T√¨m ki·∫øm trong l·ªãch s·ª≠ tr√≤ chuy·ªán")
        if search_query:
            search_results = search_chat_history(search_query)
            if search_results:
                st.write("K·∫øt qu·∫£ t√¨m ki·∫øm:")
                for result in search_results:
                    st.write(f"{result['role']}: {result['content'][:100]}...")
            else:
                st.write("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£.")

# N·ªôi dung ch√≠nh (v·ªõi c√°c c·∫£i ti·∫øn)
st.title("üöÄ Gemini Agent")
st.caption("Tr·∫£i nghi·ªám s·ª©c m·∫°nh c·ªßa c√°c m√¥ h√¨nh Gemini m·ªõi nh·∫•t v·ªõi t√πy ch·ªânh n√¢ng cao. üåü")

st.write(f"Phi√™n hi·ªán t·∫°i: {st.session_state.current_session}")

if api_key:
    if len(api_key) > 10:  # Ki·ªÉm tra ƒë∆°n gi·∫£n v·ªÅ t√≠nh h·ª£p l·ªá c·ªßa API key
        st.session_state.api_key = api_key
        model = load_model(api_key, st.session_state.model_config["model_name"])
        
        if model:
            # Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
            for msg in st.session_state.chat_history:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])
                    timestamp = msg.get('timestamp', 'N/A')
                    tokens = msg.get('tokens', 'N/A')
                    processing_time = msg.get('processing_time', None)
                    
                    info_text = f"{timestamp} | Tokens: {tokens}"
                    if processing_time and msg["role"] == "assistant":
                        formatted_time = f"{processing_time:.1f}s"
                        info_text += f" | {formatted_time}"
                    
                    st.markdown(f"<div class='timestamp'>{info_text}</div>", unsafe_allow_html=True)

            # X·ª≠ l√Ω input c·ªßa ng∆∞·ªùi d√πng
            prompt = st.chat_input("üí¨ B·∫°n mu·ªën bi·∫øt g√¨?")
            if prompt:
                user_timestamp = datetime.now().strftime("%H:%M:%S")
                user_tokens = count_tokens(prompt)
                
                with st.chat_message("user"):
                    st.markdown(sanitize_input(prompt))
                    st.markdown(f"<div class='timestamp'>{user_timestamp} | Tokens: {user_tokens}</div>", unsafe_allow_html=True)
                
                st.session_state.total_tokens += user_tokens
                st.session_state.chat_history.append({
                    "role": "user",
                    "content": sanitize_input(prompt),
                    "tokens": user_tokens,
                    "timestamp": user_timestamp
                })

                with st.chat_message("assistant"):
                    response, response_tokens, processing_time = handle_user_input(prompt, model)
                    if response:
                        st.markdown(response)
                        timestamp = datetime.now().strftime("%H:%M:%S")
                        formatted_time = f"{processing_time:.1f}s"
                        st.markdown(f"<div class='timestamp'>{timestamp} | Tokens: {response_tokens} | {formatted_time}</div>", unsafe_allow_html=True)
                        
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "content": response,
                            "tokens": response_tokens,
                            "timestamp": timestamp,
                            "processing_time": processing_time
                        })

            # C·∫£nh b√°o n·∫øu c√≥ h√¨nh ·∫£nh nh∆∞ng kh√¥ng c√≥ prompt
            if st.session_state.image and not prompt:
                st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p c√¢u h·ªèi ƒë·ªÉ ƒëi k√®m v·ªõi h√¨nh ·∫£nh.")
        else:
            handle_error("Kh√¥ng th·ªÉ kh·ªüi t·∫°o m√¥ h√¨nh. Vui l√≤ng ki·ªÉm tra API key v√† th·ª≠ l·∫°i.")
    else:
        handle_error("API key kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p m·ªôt Google API Key h·ª£p l·ªá.")
else:
    st.warning("üîë Vui l√≤ng nh·∫≠p Google API Key c·ªßa b·∫°n ·ªü thanh b√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán.")

# Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì th·ªëng k√™
if st.button("üìä Hi·ªÉn th·ªã th·ªëng k√™"):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # Bi·ªÉu ƒë·ªì ph√¢n b·ªë tin nh·∫Øn
    roles = [msg['role'] for msg in st.session_state.chat_history]
    role_counts = {role: roles.count(role) for role in set(roles)}
    ax1.pie(role_counts.values(), labels=role_counts.keys(), autopct='%1.1f%%')
    ax1.set_title('Ph√¢n b·ªë tin nh·∫Øn')
    
    # Bi·ªÉu ƒë·ªì s·ª≠ d·ª•ng token
    tokens = [msg.get('tokens', 0) for msg in st.session_state.chat_history]
    ax2.plot(range(len(tokens)), tokens)
    ax2.set_title('S·ª≠ d·ª•ng token theo th·ªùi gian')
    ax2.set_xlabel('S·ªë th·ª© t·ª± tin nh·∫Øn')
    ax2.set_ylabel('S·ªë token')
    
    st.pyplot(fig)

# Footer
st.markdown("---")
st.markdown("ƒê∆∞·ª£c ph√°t tri·ªÉn v·ªõi ‚ù§Ô∏è b·ªüi tanbaycu")