import streamlit as st
import google.generativeai as genai
from PIL import Image
import io
import base64
from datetime import datetime
import tiktoken
import json
import time
import re

# C·∫•u h√¨nh trang
st.set_page_config(page_title="Tr√≤ chuy·ªán ƒêa ph∆∞∆°ng ti·ªán v·ªõi Gemini", layout="wide", page_icon="üöÄ")

# Kh·ªüi t·∫°o tr·∫°ng th√°i phi√™n
if "messages" not in st.session_state:
    st.session_state.messages = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "image" not in st.session_state:
    st.session_state.image = None
if "model_config" not in st.session_state:
    st.session_state.model_config = {
        "model_name": "gemini-1.5-flash-latest",
        "temperature": 0.7,
        "top_p": 1.0,
        "top_k": 40,
        "max_output_tokens": 2048,
    }
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch v√† th√¢n thi·ªán ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi tanbaycu. "
if "total_tokens" not in st.session_state:
    st.session_state.total_tokens = 0
if "theme" not in st.session_state:
    st.session_state.theme = "light"
if "font_size" not in st.session_state:
    st.session_state.font_size = "medium"

GEMINI_MODELS = [
    "gemini-1.5-flash-latest",
    "gemini-2.0-flash-exp",
    "gemini-1.5-flash-8b",
    "gemini-2.0-pro-exp-02-05",
    "gemini-2.0-flash-lite-preview-02-05"
]

MAX_TOKENS = 8192  # Gi·∫£ s·ª≠ ƒë√¢y l√† gi·ªõi h·∫°n token cho m√¥ h√¨nh

# CSS t√πy ch·ªânh
def get_custom_css():
    return f"""
<style>
    .stButton > button {{width: 100%;}}
    .stTextInput > div > div > input {{background-color: {'#f0f2f6' if st.session_state.theme == 'light' else '#2b313e'};}}
    .sidebar .stButton > button {{background-color: #4CAF50; color: white;}}
    .sidebar .stButton > button:hover {{background-color: #45a049;}}
    .chat-message {{
        padding: 1rem; 
        border-radius: 0.5rem; 
        margin-bottom: 1rem; 
        display: flex;
        font-size: {{'0.8rem' if st.session_state.font_size == 'small' else '1rem' if st.session_state.font_size == 'medium' else '1.2rem'}};
        animation: fadeIn 0.5s;
    }}
    @keyframes fadeIn {{
        0% {{ opacity: 0; }}
        100% {{ opacity: 1; }}
    }}
    .chat-message.user {{background-color: {'#e6f3ff' if st.session_state.theme == 'light' else '#2b313e'};}}
    .chat-message.bot {{background-color: {'#f0f0f0' if st.session_state.theme == 'light' else '#3c4354'};}}
    .chat-message .avatar {{width: 15%; padding-right: 0.5rem;}}
    .chat-message .avatar img {{max-width: 40px; max-height: 40px; border-radius: 50%;}}
    .chat-message .message {{width: 85%; padding: 0 1.5rem;}}
    .chat-message .timestamp {{font-size: 0.8em; color: {'#a0a0a0' if st.session_state.theme == 'light' else '#cccccc'}; text-align: right; margin-top: 0.5rem;}}
    .token-info {{font-size: 0.8em; color: {'#a0a0a0' if st.session_state.theme == 'light' else '#cccccc'}; margin-top: 0.5rem;}}
    body {{background-color: {'#ffffff' if st.session_state.theme == 'light' else '#1e1e1e'}; color: {'#000000' if st.session_state.theme == 'light' else '#ffffff'};}}
    .stAlert {{animation: slideIn 0.5s;}}
    @keyframes slideIn {{
        0% {{ transform: translateY(-100%); }}
        100% {{ transform: translateY(0); }}
    }}
</style>
"""

st.markdown(get_custom_css(), unsafe_allow_html=True)

# C√°c h√†m ti·ªán √≠ch
@st.cache_resource
def load_model(api_key, model_name):
    try:
        genai.configure(api_key=api_key)
        return genai.GenerativeModel(model_name=model_name)
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o m√¥ h√¨nh: {str(e)}")
        return None

@st.cache_data
def get_image_download_link(_img, filename, text):
    buffered = io.BytesIO()
    _img.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    href = f'<a href="data:file/png;base64,{img_str}" download="{filename}">{text}</a>'
    return href

def get_chat_history():
    return "\n".join([f"{msg['role']}: {msg['content']}" for msg in st.session_state.chat_history[-st.session_state.max_history:]])

@st.cache_data
def count_tokens(text):
    encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

# Th√™m h√†m m·ªõi ƒë·ªÉ x·ª≠ l√Ω gi·ªõi h·∫°n t·ªëc ƒë·ªô
def rate_limited_response(func):
    def wrapper(*args, **kwargs):
        if 'last_request_time' not in st.session_state:
            st.session_state.last_request_time = 0
        
        current_time = time.time()
        time_since_last_request = current_time - st.session_state.last_request_time
        
        if time_since_last_request < 1:  # Gi·ªõi h·∫°n 1 y√™u c·∫ßu m·ªói gi√¢y
            time.sleep(1 - time_since_last_request)
        
        result = func(*args, **kwargs)
        st.session_state.last_request_time = time.time()
        return result
    return wrapper

@rate_limited_response
def handle_user_input(user_input, model):
    start_time = time.time()
    sanitized_input = sanitize_input(user_input)
    chat_history = get_chat_history()
    full_prompt = f"{st.session_state.system_prompt}\n\nL·ªãch s·ª≠ tr√≤ chuy·ªán:\n{chat_history}\n\nNg∆∞·ªùi d√πng: {sanitized_input}\n\nTr·ª£ l√Ω:"
    
    inputs = [full_prompt]
    if st.session_state.image:
        inputs.append(st.session_state.image)
    
    try:
        with st.spinner('ü§î ƒêang t·∫°o ph·∫£n h·ªìi...'):
            response = model.generate_content(
                inputs,
                generation_config=genai.types.GenerationConfig(
                    temperature=st.session_state.model_config["temperature"],
                    top_p=st.session_state.model_config["top_p"],
                    top_k=st.session_state.model_config["top_k"],
                    max_output_tokens=st.session_state.model_config["max_output_tokens"],
                )
            )
        response_tokens = count_tokens(response.text)
        st.session_state.total_tokens += response_tokens
        processing_time = time.time() - start_time
        return response.text, response_tokens, processing_time
    except Exception as e:
        st.error(f"L·ªói t·∫°o ph·∫£n h·ªìi: {str(e)}")
        return None, 0, 0

def sanitize_input(text):
    # Lo·∫°i b·ªè c√°c th·∫ª HTML
    text = re.sub('<[^<]+?>', '', text)
    # Escape c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
    text = text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('"', '&quot;').replace("'", '&#39;')
    return text

def save_chat_session():
    session_data = {
        "messages": st.session_state.messages,
        "chat_history": st.session_state.chat_history,
        "total_tokens": st.session_state.total_tokens
    }
    return json.dumps(session_data)

def load_chat_session(session_data):
    data = json.loads(session_data)
    st.session_state.messages = data["messages"]
    st.session_state.chat_history = data["chat_history"]
    st.session_state.total_tokens = data["total_tokens"]

def is_valid_api_key(api_key):
    # ƒê√¢y l√† ki·ªÉm tra c∆° b·∫£n. B·∫°n c√≥ th·ªÉ mu·ªën tri·ªÉn khai m·ªôt x√°c th·ª±c m·∫°nh m·∫Ω h∆°n.
    return bool(api_key) and len(api_key) > 10

# H√†m m·ªõi ƒë·ªÉ ƒë·ªãnh d·∫°ng th·ªùi gian x·ª≠ l√Ω
def format_processing_time(seconds):
    if seconds < 0.1:
        return f"{seconds*1000:.0f}ms"
    else:
        return f"{seconds:.1f}s"

# Thanh b√™n
with st.sidebar:
    st.title("C√†i ƒë·∫∑t")
    
    if 'api_key' not in st.session_state:
        st.session_state.api_key = ''
    api_key = st.text_input("Nh·∫≠p Google API Key", type="password", value=st.session_state.api_key)
    
    with st.expander("üõ†Ô∏è T√πy ch·ªânh M√¥ h√¨nh", expanded=False):
        selected_model = st.selectbox("Ch·ªçn m√¥ h√¨nh Gemini", GEMINI_MODELS, index=GEMINI_MODELS.index(st.session_state.model_config["model_name"]))
        st.session_state.model_config["model_name"] = selected_model
        
        st.session_state.model_config["temperature"] = st.slider("üå°Ô∏è ƒê·ªô s√°ng t·∫°o", min_value=0.0, max_value=1.0, value=st.session_state.model_config["temperature"], step=0.1)
        st.session_state.model_config["top_p"] = st.slider("üéØ Top P", min_value=0.0, max_value=1.0, value=st.session_state.model_config["top_p"], step=0.1)
        st.session_state.model_config["top_k"] = st.number_input("üîù Top K", min_value=1, max_value=100, value=st.session_state.model_config["top_k"])
        st.session_state.model_config["max_output_tokens"] = st.number_input("üìè S·ªë token t·ªëi ƒëa", min_value=1, max_value=8192, value=st.session_state.model_config["max_output_tokens"])
    
    with st.expander("üìù T√πy ch·ªânh Prompt", expanded=False):
        st.session_state.system_prompt = st.text_area("System Prompt", value=st.session_state.system_prompt, height=100)
    
    st.session_state.max_history = st.slider("üß† S·ªë l∆∞·ª£ng tin nh·∫Øn t·ªëi ƒëa trong l·ªãch s·ª≠", min_value=1, max_value=20, value=5)
    
    uploaded_file = st.file_uploader("üì∏ T·∫£i l√™n m·ªôt h√¨nh ·∫£nh...", type=["jpg", "jpeg", "png"])

    if uploaded_file:
        st.session_state.image = Image.open(uploaded_file)
        st.image(st.session_state.image, caption='H√¨nh ·∫£nh ƒë√£ t·∫£i l√™n', use_column_width=True)
        st.markdown(get_image_download_link(st.session_state.image, "h√¨nh_·∫£nh_ƒë√£_t·∫£i.png", "üì• T·∫£i xu·ªëng h√¨nh ·∫£nh"), unsafe_allow_html=True)

    # Menu hamburger cho c√°c t√πy ch·ªçn b·ªï sung
    with st.expander("‚ò∞ T√πy ch·ªçn n√¢ng cao", expanded=False):
        st.subheader("Qu·∫£n l√Ω phi√™n tr√≤ chuy·ªán")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üóëÔ∏è X√≥a", key="clear_history"):
                st.session_state.messages = []
                st.session_state.chat_history = []
                st.session_state.image = None
                st.session_state.total_tokens = 0
                st.rerun()

        with col2:
            if st.button("üì• Xu·∫•t", key="export_history"):
                chat_history = "\n".join([f"{msg['role']} ({msg.get('timestamp', 'N/A')}): {msg['content']}" for msg in st.session_state.chat_history])
                st.download_button(
                    label="üì• T·∫£i xu·ªëng",
                    data=chat_history,
                    file_name="lich_su_tro_chuyen.txt",
                    mime="text/plain"
                )
        
        st.subheader("L∆∞u v√† t·∫£i phi√™n tr√≤ chuy·ªán")
        if st.button("üíæ L∆∞u"):
            session_data = save_chat_session()
            st.download_button(
                label="üì• T·∫£i xu·ªëng phi√™n tr√≤ chuy·ªán",
                data=session_data,
                file_name="phien_tro_chuyen.json",
                mime="application/json"
            )
        
        uploaded_session = st.file_uploader("üì§ T·∫£i l√™n", type=["json"])
        if uploaded_session is not None:
            session_data = uploaded_session.getvalue().decode("utf-8")
            load_chat_session(session_data)
            st.success("ƒê√£ t·∫£i phi√™n tr√≤ chuy·ªán th√†nh c√¥ng!")

        st.subheader("Th√¥ng tin M√¥ h√¨nh")
        st.info(f"""
        - ü§ñ M√¥ h√¨nh: {st.session_state.model_config['model_name']}
        - üå°Ô∏è ƒê·ªô s√°ng t·∫°o: {st.session_state.model_config['temperature']:.2f}
        - üéØ Top P: {st.session_state.model_config['top_p']:.2f}
        - üîù Top K: {st.session_state.model_config['top_k']}
        - üìè S·ªë token t·ªëi ƒëa: {st.session_state.model_config['max_output_tokens']}
        - üß† S·ªë l∆∞·ª£ng tin nh·∫Øn trong l·ªãch s·ª≠: {st.session_state.max_history}
        - üí¨ T·ªïng s·ªë tin nh·∫Øn: {len(st.session_state.messages)}
        - üî¢ T·ªïng s·ªë token: {st.session_state.total_tokens}
        """)

        # Thanh ti·∫øn tr√¨nh token
        st.subheader("S·ª≠ d·ª•ng Token")
        progress = st.session_state.total_tokens / MAX_TOKENS
        st.progress(progress)
        st.text(f"{st.session_state.total_tokens}/{MAX_TOKENS} tokens ƒë√£ s·ª≠ d·ª•ng")

# N·ªôi dung ch√≠nh
st.title("üöÄ Gemini Agent")
st.caption("Tr·∫£i nghi·ªám s·ª©c m·∫°nh c·ªßa c√°c m√¥ h√¨nh Gemini m·ªõi nh·∫•t v·ªõi t√πy ch·ªânh n√¢ng cao. üåü")

if api_key:
    if is_valid_api_key(api_key):
        st.session_state.api_key = api_key
        model = load_model(api_key, st.session_state.model_config["model_name"])
        
        if model:
            # Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
            for msg in st.session_state.chat_history:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])
                    timestamp = msg.get('timestamp', 'N/A')
                    tokens = msg.get('tokens', 'N/A')
                    processing_time = msg.get('processing_time', None)
                    
                    info_text = f"{timestamp} | Tokens: {tokens}"
                    if processing_time and msg["role"] == "assistant":
                        formatted_time = format_processing_time(processing_time)
                        info_text += f" | {formatted_time}"
                    
                    st.markdown(f"<div class='timestamp'>{info_text}</div>", unsafe_allow_html=True)

            # X·ª≠ l√Ω input c·ªßa ng∆∞·ªùi d√πng
            prompt = st.chat_input("üí¨ B·∫°n mu·ªën bi·∫øt g√¨?")
            if prompt:
                user_timestamp = datetime.now().strftime("%H:%M:%S")
                user_tokens = count_tokens(prompt)
                
                with st.chat_message("user"):
                    st.markdown(sanitize_input(prompt))
                    st.markdown(f"<div class='timestamp'>{user_timestamp} | Tokens: {user_tokens}</div>", unsafe_allow_html=True)
                
                st.session_state.total_tokens += user_tokens
                st.session_state.chat_history.append({
                    "role": "user",
                    "content": sanitize_input(prompt),
                    "tokens": user_tokens,
                    "timestamp": user_timestamp
                })

                with st.chat_message("assistant"):
                    response_start_time = time.time()
                    response, response_tokens, processing_time = handle_user_input(prompt, model)
                    if response:
                        st.markdown(response)
                        timestamp = datetime.now().strftime("%H:%M:%S")
                        formatted_time = format_processing_time(processing_time)
                        total_time = format_processing_time(time.time() - response_start_time)
                        st.markdown(f"<div class='timestamp'>{timestamp} | Tokens: {response_tokens} | X·ª≠ l√Ω: {formatted_time} | T·ªïng: {total_time}</div>", unsafe_allow_html=True)
                        
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "content": response,
                            "tokens": response_tokens,
                            "timestamp": timestamp,
                            "processing_time": processing_time,
                            "total_time": time.time() - response_start_time
                        })

            # C·∫£nh b√°o n·∫øu c√≥ h√¨nh ·∫£nh nh∆∞ng kh√¥ng c√≥ prompt
            if st.session_state.image and not prompt:
                st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p c√¢u h·ªèi ƒë·ªÉ ƒëi k√®m v·ªõi h√¨nh ·∫£nh.")
        else:
            st.error("‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o m√¥ h√¨nh. Vui l√≤ng ki·ªÉm tra API key v√† th·ª≠ l·∫°i.")
    else:
        st.error("‚ùå API key kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p m·ªôt Google API Key h·ª£p l·ªá.")
else:
    st.warning("üîë Vui l√≤ng nh·∫≠p Google API Key c·ªßa b·∫°n ·ªü thanh b√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán.")

# Footer
st.markdown("---")
st.markdown("ƒê∆∞·ª£c ph√°t tri·ªÉn v·ªõi ‚ù§Ô∏è b·ªüi tanbaycu")

